{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr0Ca821MYHBs/BN76XpBP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kihoon71/mixed_precision_numpy/blob/main/single_precision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mixed Precision\n",
        "\n",
        "mixed precision을 numpy로 간단하게 재현하기 위한 노트북입니다.numpy로 된 MLP 코드와 학습용 데이터셋을 통해 간단하게, 재현해보도록 하겠습니다."
      ],
      "metadata": {
        "id": "nw7y_IFKJQib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Gov88apCJR_e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 해당 코드는 밑바닥부터 딥러닝과 gpt를 통해 생성된 코드입니다.\n",
        "class MLP:\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "        self.weights = [np.random.randn(layer_sizes[i], layer_sizes[i+1]) for i in range(len(layer_sizes) - 1)]\n",
        "        self.biases = [np.random.randn(1, layer_sizes[i+1]) for i in range(len(layer_sizes) - 1)]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_values / np.sum(exp_values, axis=-1, keepdims=True)\n",
        "\n",
        "    def softmax_derivative(self, x):\n",
        "        s = x.reshape(-1, 1)\n",
        "        return np.diagflat(s) - np.dot(s, s.T)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파 계산\n",
        "        activations = [x] # input x와 계산된 x가 차례대로 들어가게 됨\n",
        "        weighted_inputs = []\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            weighted_input = np.dot(activations[-1], self.weights[i]) + self.biases[i] # matmul + bias\n",
        "            weighted_inputs.append(weighted_input)\n",
        "            if i == len(self.weights) - 1: # 마지막 출력층일 경우\n",
        "                activation = self.softmax(weighted_input)\n",
        "            else: # 활성화 함수\n",
        "                activation = self.sigmoid(weighted_input)\n",
        "            activations.append(activation)\n",
        "\n",
        "        return activations, weighted_inputs\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        # 크로스 엔트로피 손실 계산\n",
        "        return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "    def compute_output_error(self, y_true, y_pred):\n",
        "        # 출력층 오차 계산\n",
        "        return y_true - y_pred\n",
        "\n",
        "    def compute_hidden_error(self, next_layer_error, next_layer_weights, current_layer_output):\n",
        "        # 은닉층 오차 계산\n",
        "        return np.dot(next_layer_error, next_layer_weights.T) * self.sigmoid_derivative(current_layer_output)\n",
        "\n",
        "    def update_weights(self, activations, errors, learning_rate):\n",
        "        # 가중치 업데이트\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] += learning_rate * activations[i].T.dot(errors[i])\n",
        "            self.biases[i] += learning_rate * np.sum(errors[i], axis=0)\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for i in range(len(X)):\n",
        "                x = X[i:i+1]\n",
        "                target = y[i:i+1]\n",
        "\n",
        "                # 순전파\n",
        "                activations, weighted_inputs = self.forward(x)\n",
        "\n",
        "                # 오차 계산\n",
        "                output_error = self.compute_output_error(target, activations[-1])\n",
        "\n",
        "\n",
        "                # 역전파를 위한 오차들 초기화\n",
        "                errors = [output_error]\n",
        "\n",
        "                # 은닉층 오차 계산\n",
        "                for j in range(len(self.weights) - 1, 0, -1):\n",
        "                    error = self.compute_hidden_error(errors[-1], self.weights[j], activations[j])\n",
        "                    errors.append(error)\n",
        "\n",
        "                errors.reverse()\n",
        "\n",
        "                # 가중치 업데이트\n",
        "                self.update_weights(activations, errors, learning_rate)\n",
        "\n",
        "                # 손실 계산\n",
        "                loss = self.compute_loss(target, activations[-1])\n",
        "                total_loss += loss\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {total_loss / len(X)}\")\n",
        "\n",
        "    def predict(self, x):\n",
        "        activations, _ = self.forward(x)\n",
        "        return activations[-1]\n"
      ],
      "metadata": {
        "id": "DZifPNAWMS67"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 불러오기\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "# 필요한 특성 선택\n",
        "selected_features = list(map(lambda x: x.lower(), ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex', 'Embarked', 'Survived']))\n",
        "df = df[selected_features]\n",
        "\n",
        "# 결측치 처리\n",
        "df = df.dropna()\n",
        "\n",
        "# 범주형 데이터 처리 (One-hot encoding)\n",
        "df = pd.get_dummies(df, columns=['sex', 'embarked'])\n",
        "\n",
        "# 입력(X)과 출력(y) 분리\n",
        "X = df.drop('survived', axis=1).values\n",
        "y = pd.get_dummies(df['survived']).values  # 생존 여부를 One-hot encoding으로 변환\n",
        "\n",
        "# 데이터 정규화\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# 데이터 분할\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# MLP 모델 생성\n",
        "mlp = MLP(input_size=X.shape[1], hidden_sizes=[8], output_size=y.shape[1])  # 출력층 크기는 클래스 개수에 맞게 설정\n",
        "\n",
        "# 학습\n",
        "mlp.train(X_train, y_train, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "# 테스트 데이터에 대한 예측\n",
        "predictions = mlp.predict(X_test)\n",
        "\n",
        "# 예측 정확도 계산\n",
        "accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n",
        "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNvsW1FcNz-C",
        "outputId": "2699eda8-ec58-4311-e627-dc09601b5a7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
            "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
            "       'alive', 'alone'],\n",
            "      dtype='object')\n",
            "Epoch 0, Loss: 0.5358807230140038\n",
            "Epoch 100, Loss: 0.36140190917683623\n",
            "Epoch 200, Loss: 0.3451140471529232\n",
            "Epoch 300, Loss: 0.3363091962599866\n",
            "Epoch 400, Loss: 0.33135929390979374\n",
            "Epoch 500, Loss: 0.32818483634017204\n",
            "Epoch 600, Loss: 0.3259232883381506\n",
            "Epoch 700, Loss: 0.32400008998840457\n",
            "Epoch 800, Loss: 0.3221611464043902\n",
            "Epoch 900, Loss: 0.32053412163797135\n",
            "Accuracy on test set: 75.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDKsg5jTN4i9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}